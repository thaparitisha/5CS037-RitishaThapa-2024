{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##3.1.1 Step -1- Data Understanding, Analysis and Preparations:\n",
        "\n",
        "• To - Do - 1:\n",
        "1. Read and Observe the Dataset.\n",
        "2. Print top(5) and bottom(5) of the dataset {Hint: pd.head and pd.tail}.\n",
        "3. Print the Information of Datasets. {Hint: pd.info}.\n",
        "4. Gather the Descriptive info about the Dataset. {Hint: pd.describe}\n",
        "5. Split your data into Feature (X) and Label (Y).\n",
        "\n",
        "• To - Do - 2:\n",
        "\n",
        "1. To make the task easier - let’s assume there is no bias or intercept.\n",
        "2. Create the following matrices:\n",
        "\n",
        "Y = WTX\n",
        "\n",
        "• To - Do - 3:\n",
        "1. Split the dataset into training and test sets.\n",
        "2. You can use an 80-20 or 70-30 split, with 80% (or 70%) of the data used for training and the rest\n",
        "for testing."
      ],
      "metadata": {
        "id": "k_JpKddHQZN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataset_path = '/content/student.csv'\n",
        "data = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "id": "D2jFmOOZSneS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXUPt2MGQENL",
        "outputId": "76028f97-4002-4ecd-8430-e976df2b74bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 rows of the dataset:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "Bottom 5 rows of the dataset:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "\n",
            "Descriptive Statistics:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Print Top (5) and Bottom (5) of the Dataset\n",
        "print(\"Top 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "print(\"\\nBottom 5 rows of the dataset:\")\n",
        "print(data.tail())\n",
        "\n",
        "#Print Information of the Dataset\n",
        "print(\"\\nDataset Information:\")\n",
        "data.info()\n",
        "\n",
        "#Gather Descriptive Info about the Dataset\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(data.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into features (X) and label (Y)\n",
        "X = data[['Math', 'Reading']]\n",
        "Y = data['Writing']\n",
        "\n",
        "print(\"Features (X):\")\n",
        "print(X.head())\n",
        "\n",
        "print(\"Label (Y):\")\n",
        "print(Y.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXyCA3oRiF5t",
        "outputId": "cc91df0f-7a94-42c9-8f3d-f3eb5f048001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features (X):\n",
            "   Math  Reading\n",
            "0    48       68\n",
            "1    62       81\n",
            "2    79       80\n",
            "3    76       83\n",
            "4    59       64\n",
            "Label (Y):\n",
            "0    63\n",
            "1    72\n",
            "2    78\n",
            "3    79\n",
            "4    62\n",
            "Name: Writing, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['Math', 'Reading']].values\n",
        "Y = data['Writing'].values\n"
      ],
      "metadata": {
        "id": "qznWkoYxiF9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Create matrices X and Y from the dataset\n",
        "# # Assuming 'Math' and 'Reading' are the feature columns and 'Writing' is the label column\n",
        "# X = data[['Math', 'Reading']].to_numpy().T  # Transpose to get X in the correct shape (d x n)\n",
        "# Y = data['Writing'].to_numpy().reshape(1, -1)  # Reshape to get Y in the correct shape (1 x n)\n",
        "\n",
        "# # Step 3: Initialize the weights matrix W with random values\n",
        "# d = X.shape[0]  # Number of features\n",
        "# W = np.random.rand(d, 1)\n",
        "\n",
        "# # Printing the matrices\n",
        "# print(\"Matrix W (weights):\")\n",
        "# print(W)\n",
        "\n",
        "# print(\"\\nMatrix X (features):\")\n",
        "# print(X)\n",
        "\n",
        "# print(\"\\nMatrix Y (labels):\")\n",
        "# print(Y)\n",
        "x_t=X.T\n",
        "W = np.linalg.inv(x_t.dot(X)).dot(x_t).dot(Y)\n",
        "print(\"weight vector(w):\",W)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHOp4TXwS5xD",
        "outputId": "746c98f8-b760-4386-8781-994eb402381a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight vector(w): [0.0891154  0.89704474]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# X = data[['Math', 'Reading']].values\n",
        "# y = data['Writing'].values\n",
        "# # Define a function for train-test split from scratch\n",
        "# def train_test_split_scratch(X, y, test_size=0.3, random_seed=42):\n",
        "#   np.random.seed(random_seed)\n",
        "#   indices = np.arange(X.shape[0])\n",
        "#   np.random.shuffle(indices) # Shuffle the indices\n",
        "#   test_split_size = int(len(X) * test_size)\n",
        "#   test_indices = indices[:test_split_size]\n",
        "#   train_indices = indices[test_split_size:]\n",
        "#   X_train, X_test = X[train_indices], X[test_indices]\n",
        "#   y_train, y_test = y[train_indices], y[test_indices]\n",
        "#   return X_train, X_test, y_train, y_test\n",
        "\n",
        "# # Perform the train-test split\n",
        "# X_train, X_test, y_train, y_test = train_test_split_scratch(X, y, test_size=0.3)\n",
        "# # Output shapes to verify\n",
        "# print(\"Shape of X_train:\", X_train.shape)\n",
        "# print(\"Shape of X_test:\", X_test.shape)\n",
        "# print(\"Shape of y_train:\", y_train.shape)\n",
        "# print(\"Shape of y_test:\", y_test.shape)\n",
        "\n",
        "y_pred=X.dot(W)\n",
        "print(\"Predicitons:\",y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1heReYQyiQE",
        "outputId": "c821cd32-adae-407f-8b2d-70a58a85271f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicitons: [65.27658143 78.18577863 78.80369567 81.22748369 62.66867186 81.50072064\n",
            " 81.58983604 47.15745584 75.40552901 77.63341399 67.42124175 85.35624579\n",
            " 44.46043088 77.18783699 68.2291711  43.83662308 69.03710044 47.06834045\n",
            " 87.06121987 73.96201038 57.91021122 64.28453054 70.55795297 52.26059659\n",
            " 83.20569472 86.79387367 64.73010754 72.2688178  77.27695239 79.26105416\n",
            " 75.39963826 86.88298907 72.08469625 68.66885734 42.21487366 58.90815285\n",
            " 51.99914114 75.75609986 72.53027325 43.21281529 84.54831645 76.02344605\n",
            " 61.40927478 48.50007758 66.43508162 70.73618377 52.08825654 72.08469625\n",
            " 97.72485985 71.55000385 98.61601384 86.34240593 78.18577863 49.75947466\n",
            " 49.04066072 48.67830838 54.8626154  49.21889152 68.75797274 78.89870182\n",
            " 54.68438461 89.83557874 96.82192436 80.8710221  44.09807853 90.28115574\n",
            " 65.98950462 83.00979167 59.8825315  57.91021122 60.77957624 58.18344816\n",
            " 47.69214824 61.86074252 63.654832   51.64267954 65.62715228 44.83456472\n",
            " 75.57197831 64.45687059 80.86513135 78.36400942 69.66679898 75.39963826\n",
            " 44.56132777 63.55982585 84.45920105 77.37195854 77.10461234 80.24132355\n",
            " 50.56151325 71.8114593  69.93414517 70.01147908 64.37364594 67.6053633\n",
            " 59.98342839 62.21720412 97.19016745 86.42563058 30.74919509 52.08825654\n",
            " 74.23524732 53.08619817 38.81670699 77.37195854 74.58581817 43.65839229\n",
            " 76.5640292  46.26630185 67.9618249  79.61751576 85.3444643  88.31472621\n",
            " 46.89010965 81.41160524 54.60115996 74.32436272 85.2553489  74.67493357\n",
            " 68.58563269 78.80369567 81.94629763 65.52625538 47.60892359 47.33568664\n",
            " 37.7473222  54.95762155 67.85503725 82.83156088 75.03728592 77.74020163\n",
            " 66.70242781 55.13585235 54.77350001 78.71458027 79.24338192 79.60573426\n",
            " 87.59591227 83.37214402 64.10629974 96.20400731 69.75002363 67.77770335\n",
            " 65.09245988 82.04130378 66.69064632 97.72485985 76.20167685 59.80519759\n",
            " 60.69046084 83.28891937 69.75002363 56.65670488 60.24488384 46.97922505\n",
            " 87.32856607 72.35204245 62.40721641 71.8114593  80.9601375  66.26274157\n",
            " 39.62463633 73.97379188 69.29855588 72.1679209  85.70092589 72.26292705\n",
            " 67.7835941  94.13079014 57.64875577 74.23524732 63.82717205 58.72403131\n",
            " 71.63911925 89.30088635 83.73449636 61.67662097 75.40552901 63.03102421\n",
            " 65.71037693 57.46463422 64.81333219 62.12808872 67.3380171  80.6927913\n",
            " 76.1184522  83.47893166 80.502779   84.26918876 57.37551882 88.74852171\n",
            " 61.67662097 68.57385119 52.99119202 73.51643338 86.15239364 67.05888941\n",
            " 73.78967033 74.49670277 86.87120758 69.57179283 62.66867186 67.23122946\n",
            " 84.8988873  46.6286542  58.53990976 48.68419913 72.88673484 63.57160735\n",
            " 56.92405108 59.34194835 74.67493357 54.33381376 65.89449847 94.13079014\n",
            " 44.82867397 90.82762963 60.78546698 83.10479782 52.97941053 72.70850404\n",
            " 62.93012731 77.18783699 80.4254451  77.01549694 65.72215842 71.7223439\n",
            " 63.2983704  73.86700423 53.25853822 61.23104398 75.48286291 73.61143953\n",
            " 78.72047102 85.62359199 80.50866975 41.42461656 69.21533123 79.24927267\n",
            " 73.07085639 33.25620776 69.66679898 47.87037904 63.4766012  79.61751576\n",
            " 47.78126364 32.27593837 62.12808872 42.6781229  91.89701442 61.49839018\n",
            " 51.55356414 82.39776538 84.37008565 78.00165708 39.54141168 73.61143953\n",
            " 72.88673484 62.39543492 63.9221782  63.11424886 49.56946237 18.73704263\n",
            " 63.11424886 83.90683641 77.02138769 68.94798504 71.27087616 61.05870393\n",
            " 95.02783488 47.06834045 66.07272927 42.4998921  79.07104187 59.3478391\n",
            " 58.18344816 79.24927267 67.23122946 66.79154321 70.01736983 53.35354437\n",
            " 49.84859006 63.82717205 73.51643338 85.17212425 72.79761944 44.90600787\n",
            " 55.50409544 62.13397947 66.33418472 75.57786906 65.26479993 68.40151114\n",
            " 53.51999367 53.43087827 48.95154532 41.95341821 76.92049079 58.45079436\n",
            " 92.25347601 89.56234179 62.30631952 45.63071256 66.79154321 56.56169874\n",
            " 66.34007547 44.64455242 78.44723408 83.64538096 55.67643549 89.39000174\n",
            " 59.26461445 84.18596411 75.57786906 73.69466418 72.44115785 76.5640292\n",
            " 67.06478016 64.54598599 53.07441667 63.64894125 69.12032509 68.93620354\n",
            " 63.73805665 74.68082432 81.48893914 70.29649752 76.1957861  78.70868952\n",
            " 81.76806684 68.94209429 63.7439474  54.33381376 94.84960408 64.29042129\n",
            " 70.01736983 60.95780703 89.92469414 54.77939075 79.33838807 58.81314671\n",
            " 91.26142513 68.41329264 58.90815285 76.73636925 95.30107183 69.21533123\n",
            " 63.29247965 76.11256145 76.75404149 31.46211828 94.05345624 62.75189651\n",
            " 88.66529706 33.44032931 73.33231184 83.11068857 58.9913775  76.74815074\n",
            " 82.66511157 81.93451614 53.60910907 41.68018126 57.82109582 61.13603783\n",
            " 47.15745584 73.15408104 66.07272927 68.22328035 85.88504744 65.62126153\n",
            " 67.77770335 76.11256145 69.29855588 59.98342839 63.4766012  71.00352996\n",
            " 81.40571449 83.81772101 84.00184256 79.33249732 69.39356203 79.88486196\n",
            " 70.81940842 64.72421679 71.18765151 55.2308585  68.40740189 82.74833623\n",
            " 60.15576844 73.52232413 44.55543703 81.76217609 54.59526921 55.4982047\n",
            " 74.40758737 73.61143953 85.35624579 65.80538307 97.90309065 63.5657166\n",
            " 73.42731798 62.22309487 65.79949233 79.61162501 73.42731798 47.2406805\n",
            " 65.90038922 61.23104398 90.45938653 91.43965592 64.55187674 78.53634947\n",
            " 62.12219797 55.0408462  40.34345027 78.80369567 94.13668089 71.8114593\n",
            " 63.39337655 62.58544721 76.02344605 47.0624497  63.28658891 96.01988577\n",
            " 51.01887175 39.89787328 80.69868205 86.87120758 71.89468395 38.90582239\n",
            " 72.51849175 78.61957413 55.1417431  73.15997179 85.26123964 94.05345624\n",
            " 69.12032509 38.81670699 85.98005359 81.67306069 52.34971199 93.32875155\n",
            " 71.90646545 78.71458027 80.6810098  56.12790324 67.32034486 58.71814056\n",
            " 51.81501959 65.71037693 71.09264536 58.9913775  45.36336637 83.11068857\n",
            " 45.00690477 86.24739978 72.98174099 66.06094778 61.86074252 50.48417935\n",
            " 48.40507143 47.1515651  63.91628745 82.84923312 69.39356203 75.39374751\n",
            " 63.654832   86.87709833 65.97772312 74.67493357 79.35016956 60.59545469\n",
            " 74.22935658 82.57599618 63.91628745 52.61705818 61.32015938 54.51204456\n",
            " 79.96808661 58.09433277 70.28471602 69.12032509 76.21345835 77.18783699\n",
            " 47.24657124 78.53634947 50.65062865 70.81940842 79.70074041 71.63911925\n",
            " 57.55374962 96.47135351 83.28302862 77.38374003 67.23712021 86.16417513\n",
            " 50.73974405 54.14969221 68.49651729 37.11173291 58.36167896 63.83895355\n",
            " 57.01905723 65.44303073 69.83913903 86.43152133 77.37195854 72.70850404\n",
            " 68.306505   54.59526921 55.58732009 48.23862213 76.29079225 41.15137961\n",
            " 59.8825315  45.72571871 81.58394529 52.80117973 80.50866975 67.51035715\n",
            " 58.35578821 74.31847198 51.28621794 58.89637136 75.84521526 73.86700423\n",
            " 69.38767128 53.69822447 47.2406805  60.51812079 82.04719453 27.96894547\n",
            " 98.17043684 68.31828649 86.25329053 54.77939075 46.26041111 81.04336215\n",
            " 58.98548676 42.40488595 61.22515323 84.18596411 76.29079225 89.30677709\n",
            " 76.4631323  96.11489192 81.40571449 79.88486196 71.45499771 56.57348023\n",
            " 91.35643127 66.88654936 67.86681875 61.14192858 75.48875366 70.01147908\n",
            " 62.48455031 44.73366782 69.39356203 77.18783699 89.55645105 40.42667493\n",
            " 76.92049079 82.12452843 56.83493568 82.47509928 62.48455031 96.47724426\n",
            " 52.27237808 64.81922294 65.61537078 71.9005747  74.32436272 65.81127382\n",
            " 65.81127382 55.49231395 67.77770335 66.96388326 86.07505973 60.15576844\n",
            " 66.78565246 55.76555089 83.11068857 69.29855588 68.59152344 74.85905512\n",
            " 41.68607201 66.96977401 60.69635158 98.61601384 51.72001344 65.25890918\n",
            " 62.40132566 63.3874858  80.95424675 88.66529706 96.82192436 64.99156299\n",
            " 71.72823465 82.75422697 70.90852382 92.43170681 61.94985792 69.84502978\n",
            " 66.79154321 62.49044106 64.28453054 68.66885734 77.72842013 92.43170681\n",
            " 72.44115785 76.18989536 50.82885945 69.74413288 80.68690055 36.03645738\n",
            " 71.36588231 69.03710044 69.66090823 32.27004762 77.46696469 52.53383353\n",
            " 69.12621583 77.63930474 96.82781511 78.89281107 80.8710221  84.10273946\n",
            " 52.90207662 81.49482989 83.11068857 94.40991784 84.80388115 95.92487962\n",
            " 68.31239575 71.62733776 83.37803477 62.48455031 52.26648733 61.49839018\n",
            " 95.12284103 63.83895355 78.61368338 68.40740189 65.35391533 77.55608008\n",
            " 68.58563269 80.14631741 88.6711878  69.29855588 81.75628534 57.37551882\n",
            " 95.39607797 75.48286291 59.5378514  77.98987558 60.86869163 79.07693262\n",
            " 59.07460216 63.3874858  97.99220604 68.94209429 73.96201038 47.96538518\n",
            " 69.58357433 90.54850193 57.01316648 62.49044106 49.74769317 97.45751365\n",
            " 38.4602454  84.00184256 77.72842013 61.59339632 59.0804929  71.18176076\n",
            " 68.1400557  74.85905512 81.1265868  81.94629763 52.26648733 70.47472832\n",
            " 97.81397525 68.49062654 51.73179494 88.30294471 46.5277573  78.54224022\n",
            " 98.61601384 42.4107767  88.65940631 57.29818492 41.76929666 22.04609389\n",
            " 63.4766012  57.55964037 63.5657166  78.63135562 49.75358391 49.31389767\n",
            " 41.42461656 90.63761733 39.54141168 60.42900539 84.19185486 59.62107605\n",
            " 76.4749138  96.64369356 62.75778726 40.88992417 88.30294471 82.12452843\n",
            " 59.44284525 71.45499771 77.09872159 69.56590208 84.63743185 65.17568453\n",
            " 76.29079225 85.61181049 59.88842224 81.58394529 84.5424257  59.35372985\n",
            " 91.36821277 63.5657166  74.59759967 70.82529917 76.65903535 54.77350001\n",
            " 42.31577055 87.78003381 83.91861791 88.13060466 74.59759967 36.93350212\n",
            " 74.77583047 53.87645527 61.76573637 79.52250961 82.48099003 97.99220604\n",
            " 62.12808872 95.92487962 80.502779   66.87476786 56.65081414 73.86700423\n",
            " 91.08908508 69.84502978 66.96388326 63.03102421 80.4136636  71.9005747\n",
            " 72.35204245 67.68858795 81.04336215 60.68457009 61.58161483 86.69886753\n",
            " 54.24469836 74.50848427 76.30257375 75.49464441 67.05888941 73.70055493\n",
            " 51.012981   18.20235023 59.62107605 90.99996968 73.33231184 68.04504955\n",
            " 60.86869163 62.31810101 72.35204245 60.77957624 77.18783699 71.09264536\n",
            " 45.98717416 87.86325846 59.71019145 73.68877343 83.29481012 60.59545469\n",
            " 46.97922505 30.93331663 73.68877343 56.03289709 64.100409   71.62733776\n",
            " 85.61181049 66.52419702 79.78985581 81.84540074 67.8727095  50.56151325\n",
            " 98.61601384 62.48455031 60.70224233 60.86869163 70.27882527 56.48436483\n",
            " 79.70074041 51.1020964  56.30613404 57.91610197 63.82717205 47.24657124\n",
            " 69.75002363 85.08300885 75.66698446 56.22290938 58.53401901 84.44741956\n",
            " 78.44723408 59.2587237  67.7835941  54.86850615 88.39206011 72.53616399\n",
            " 59.7934161  95.84165497 68.14594644 64.64688289 71.35999156 77.99576633\n",
            " 60.77957624 57.02494798 98.61601384 58.64080666 55.4090893  67.86681875\n",
            " 82.29686848 58.62313441 84.4533103  78.62546487 63.2983704  85.70092589\n",
            " 80.4254451  72.62527939 66.61331241 73.24319644 49.30800692 45.18513557\n",
            " 71.18765151 51.1020964  77.27695239 86.15239364 38.64436694 58.89637136\n",
            " 81.57805454 91.35643127 78.09666323 63.11424886 68.04504955 78.71458027\n",
            " 53.60910907 59.8825315  73.87878573 79.33838807 63.47071045 66.61331241\n",
            " 67.95593415 44.29398158 69.65501748 63.7439474  56.75171103 74.40758737\n",
            " 58.45079436 74.31847198 80.05720201 72.17381165 58.9913775  71.9896901\n",
            " 98.08132144 91.08908508 86.60975213 70.65295912 61.94985792 43.74161694\n",
            " 53.24675672 66.44097236 84.4533103  63.2983704  65.89449847 73.15408104\n",
            " 45.54159716 88.30883546 58.08844202 81.13247754 69.21533123 68.57974194\n",
            " 45.54748791 63.9221782  66.16184467 69.21533123 80.06309276 56.02700634\n",
            " 73.24319644 72.89262559 83.64538096 50.65062865 94.76637943 96.37634736\n",
            " 42.85046295 47.16334659 90.91085428 77.18194624 69.83324828 56.57348023\n",
            " 84.81566265 77.27695239 64.63510139 38.72170085 61.86074252 61.32015938\n",
            " 67.05888941 50.84064095 64.27863979 61.95574867 83.19980397 77.55018934\n",
            " 80.69868205 74.14613193 57.47052497 83.19980397 47.86448829 65.35980608\n",
            " 91.89701442 79.25516342 36.75527132 92.69905301 47.32979589 73.33820259\n",
            " 80.05720201 72.88084409 58.98548676 81.94629763 81.13247754 69.48267743\n",
            " 70.64706837 65.88860773 68.32417724 86.43152133 59.1696083  44.01485388\n",
            " 55.0408462  84.81566265 42.58311675 56.03289709 63.21514575 91.08908508\n",
            " 84.7206565  60.78546698 63.82717205 65.35980608 55.58142935 76.2075676\n",
            " 76.11256145 81.31659909 69.65501748 65.08656913 68.85297889 65.89449847\n",
            " 66.34007547 70.82529917 64.10629974 90.64350808 68.31828649 71.80556856\n",
            " 76.296683   55.77144164 70.11237597 63.3874858  79.78985581 79.61162501\n",
            " 66.42919087 82.11863768 83.02157317 52.45060888 70.91441456 59.5260699\n",
            " 60.86869163 74.76404897 64.99745373 62.31221026 63.11424886 90.01380954\n",
            " 57.55964037 50.8229687  94.77227018 63.11424886 51.81501959 72.79761944\n",
            " 83.65127171 85.97416284 80.95424675 65.08656913]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "stML2L5XRQgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1.2  Step -2- Build a Cost Function:"
      ],
      "metadata": {
        "id": "vgI7UG-ODAon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    This function finds the Mean Square Error.\n",
        "    Input parameters:\n",
        "    X: Feature Matrix (d x n)\n",
        "    Y: Target Matrix (1 x n)\n",
        "    W: Weight Matrix (d x 1)\n",
        "    Output Parameters:\n",
        "    cost: accumulated mean square error.\n",
        "    \"\"\"\n",
        "    # Number of data points\n",
        "    n = len(Y)\n",
        "\n",
        "    # Predicted values (W^T X)\n",
        "    y_pred = np.dot(X,W)\n",
        "\n",
        "    # Mean Squared Error\n",
        "    cost = np.sum((y_pred-Y)**2)/(2*n)\n",
        "\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "cMMOfZcaxEDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "if cost == 0:\n",
        "  print(\"Proceed Further\")\n",
        "else:\n",
        "  print(\"something went wrong: Reimplement a cost function\")\n",
        "print(\"Cost function output:\", cost_function(X_test, Y_test, W_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsjO8Pkn_TY-",
        "outputId": "0a020852-05ed-406a-e0c5-a4bc9f228416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n",
            "Cost function output: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1.3 Step -3- Gradient Descent for Simple Linear Regression:"
      ],
      "metadata": {
        "id": "ytPhTxLHDJW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "  \"\"\"\n",
        "  Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "  Parameters:\n",
        "  X (numpy.ndarray): Feature matrix (m x n).\n",
        "  Y (numpy.ndarray): Target vector (m x 1).\n",
        "  W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "  alpha (float): Learning rate.\n",
        "  iterations (int): Number of iterations for gradient descent.\n",
        "  Returns:\n",
        "  tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values\n",
        "  .\n",
        "  W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "  cost_history (list): History of cost values over iterations.\n",
        "  \"\"\"\n",
        "  # Initialize cost history\n",
        "  cost_history=[]\n",
        "  # Number of samples\n",
        "  n = len(Y)\n",
        "  for iteration in range(iterations):\n",
        "    # Step 1: Hypothesis Values\n",
        "    Y_pred = np.dot(X, W)\n",
        "    # Step 2: Difference between Hypothesis and Actual Y\n",
        "    loss = Y_pred - Y\n",
        "    # Step 3: Gradient Calculation\n",
        "    dw = (1 / n) * np.dot(X.T, loss)\n",
        "    # Step 4: Updating Values of W using Gradient\n",
        "    W -= alpha * dw\n",
        "    # Step 5: New Cost Value\n",
        "    cost = cost_function(X, Y, W)\n",
        "    cost_history.append(cost)\n",
        "    return W, cost_history"
      ],
      "metadata": {
        "id": "7ClJtD7sEGbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random test data\n",
        "np.random.seed(0) # For reproducibility\n",
        "X = np.random.rand(100, 3) # 100 samples, 3 features\n",
        "Y = np.random.rand(100)\n",
        "W = np.random.rand(3) # Initial guess for parameters\n",
        "# Set hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "# Test the gradient_descent function\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "# Print the final parameters and cost history\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History:\", cost_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbzPrmRGH10L",
        "outputId": "77f4695a-43d6-416e-e59e-9bbdefc172a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.3996496  0.92745322 0.09826523]\n",
            "Cost History: [0.10711197094660153]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1.4 Step -4- Evaluate the Model:"
      ],
      "metadata": {
        "id": "i7tgnrcoKGtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Root Mean Square Error:"
      ],
      "metadata": {
        "id": "O9JWg_gPLZX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(Y, Y_pred):\n",
        "  \"\"\"\n",
        "  This Function calculates the Root Mean Squres.\n",
        "  Input Arguments:\n",
        "  Y: Array of actual(Target) Dependent Varaibles.\n",
        "  Y_pred: Array of predeicted Dependent Varaibles.\n",
        "  Output Arguments:\n",
        "  rmse: Root Mean Square.\n",
        "  \"\"\"\n",
        "  rmse = np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "  return rmse"
      ],
      "metadata": {
        "id": "PMCu0BCDKM2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "R2 or Coefficient of Determination:"
      ],
      "metadata": {
        "id": "OJWubeh9LvHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - R2\n",
        "def r2(Y, Y_pred):\n",
        "  \"\"\"\n",
        "  This Function calculates the R Squared Error.\n",
        "  Input Arguments:\n",
        "  Y: Array of actual(Target) Dependent Varaibles.\n",
        "  Y_pred: Array of predeicted Dependent Varaibles.\n",
        "  Output Arguments:\n",
        "  rsquared: R Squared Error.\n",
        "  \"\"\"\n",
        "  mean_y = np.mean(Y)\n",
        "  ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "  ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "  r2 = 1 - (ss_res / ss_tot)\n",
        "\n",
        "  return r2"
      ],
      "metadata": {
        "id": "SVFIcrCmLysG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1.5 Step -5- Main Function to Integrate All Steps:"
      ],
      "metadata": {
        "id": "272audABMG3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('/content/student.csv')\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
        "    Y = data['Writing'].values  # Target: Writing marks\n",
        "\n",
        "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n",
        "    W = np.zeros(X_train.shape[1])  # Initialize weights\n",
        "    alpha = 0.0001  # Learning rate\n",
        "    iterations = 1000  # Number of iterations for gradient descent\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "IbMinSQpMKae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8c759b-b67b-419f-b1f6-d4fed3e80767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.47978325 0.50201988]\n",
            "Cost History (First 10 iterations): [17.813797177522098]\n",
            "RMSE on Test Set: 6.092665443799171\n",
            "R-Squared on Test Set: 0.8517062814522471\n"
          ]
        }
      ]
    }
  ]
}